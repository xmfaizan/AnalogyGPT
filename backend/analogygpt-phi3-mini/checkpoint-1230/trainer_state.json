{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1230,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01628001628001628,
      "grad_norm": 1.887290120124817,
      "learning_rate": 9e-06,
      "loss": 3.1492,
      "step": 10
    },
    {
      "epoch": 0.03256003256003256,
      "grad_norm": 1.9540897607803345,
      "learning_rate": 1.9e-05,
      "loss": 2.9923,
      "step": 20
    },
    {
      "epoch": 0.04884004884004884,
      "grad_norm": 0.7482802867889404,
      "learning_rate": 2.9e-05,
      "loss": 2.6187,
      "step": 30
    },
    {
      "epoch": 0.06512006512006512,
      "grad_norm": 0.9587072730064392,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 2.546,
      "step": 40
    },
    {
      "epoch": 0.0814000814000814,
      "grad_norm": 0.9035333395004272,
      "learning_rate": 4.9e-05,
      "loss": 2.1909,
      "step": 50
    },
    {
      "epoch": 0.09768009768009768,
      "grad_norm": 0.8544525504112244,
      "learning_rate": 4.961864406779661e-05,
      "loss": 1.9077,
      "step": 60
    },
    {
      "epoch": 0.11396011396011396,
      "grad_norm": 0.924026608467102,
      "learning_rate": 4.919491525423729e-05,
      "loss": 1.5132,
      "step": 70
    },
    {
      "epoch": 0.13024013024013023,
      "grad_norm": 1.056766390800476,
      "learning_rate": 4.88135593220339e-05,
      "loss": 1.1531,
      "step": 80
    },
    {
      "epoch": 0.14652014652014653,
      "grad_norm": 1.1141852140426636,
      "learning_rate": 4.8389830508474574e-05,
      "loss": 1.0166,
      "step": 90
    },
    {
      "epoch": 0.1628001628001628,
      "grad_norm": 0.8041523098945618,
      "learning_rate": 4.796610169491525e-05,
      "loss": 0.8734,
      "step": 100
    },
    {
      "epoch": 0.1790801790801791,
      "grad_norm": 0.7326760292053223,
      "learning_rate": 4.754237288135593e-05,
      "loss": 0.7764,
      "step": 110
    },
    {
      "epoch": 0.19536019536019536,
      "grad_norm": 0.5767306089401245,
      "learning_rate": 4.711864406779661e-05,
      "loss": 0.8504,
      "step": 120
    },
    {
      "epoch": 0.21164021164021163,
      "grad_norm": 0.855254590511322,
      "learning_rate": 4.669491525423729e-05,
      "loss": 0.7419,
      "step": 130
    },
    {
      "epoch": 0.22792022792022792,
      "grad_norm": 0.993162214756012,
      "learning_rate": 4.6271186440677964e-05,
      "loss": 0.7147,
      "step": 140
    },
    {
      "epoch": 0.2442002442002442,
      "grad_norm": 0.8096955418586731,
      "learning_rate": 4.5847457627118644e-05,
      "loss": 0.7442,
      "step": 150
    },
    {
      "epoch": 0.26048026048026046,
      "grad_norm": 0.7697708010673523,
      "learning_rate": 4.542372881355932e-05,
      "loss": 0.7421,
      "step": 160
    },
    {
      "epoch": 0.27676027676027676,
      "grad_norm": 0.7750405669212341,
      "learning_rate": 4.5e-05,
      "loss": 0.7419,
      "step": 170
    },
    {
      "epoch": 0.29304029304029305,
      "grad_norm": 0.7274835705757141,
      "learning_rate": 4.457627118644068e-05,
      "loss": 0.7048,
      "step": 180
    },
    {
      "epoch": 0.3093203093203093,
      "grad_norm": 0.6769528985023499,
      "learning_rate": 4.4152542372881355e-05,
      "loss": 0.723,
      "step": 190
    },
    {
      "epoch": 0.3256003256003256,
      "grad_norm": 0.7688562870025635,
      "learning_rate": 4.3728813559322035e-05,
      "loss": 0.6998,
      "step": 200
    },
    {
      "epoch": 0.3418803418803419,
      "grad_norm": 0.7896013855934143,
      "learning_rate": 4.3305084745762714e-05,
      "loss": 0.7142,
      "step": 210
    },
    {
      "epoch": 0.3581603581603582,
      "grad_norm": 1.0522710084915161,
      "learning_rate": 4.2881355932203394e-05,
      "loss": 0.6904,
      "step": 220
    },
    {
      "epoch": 0.3744403744403744,
      "grad_norm": 0.9165351390838623,
      "learning_rate": 4.245762711864407e-05,
      "loss": 0.6874,
      "step": 230
    },
    {
      "epoch": 0.3907203907203907,
      "grad_norm": 0.8769110441207886,
      "learning_rate": 4.2033898305084746e-05,
      "loss": 0.7104,
      "step": 240
    },
    {
      "epoch": 0.407000407000407,
      "grad_norm": 0.8183981776237488,
      "learning_rate": 4.1610169491525425e-05,
      "loss": 0.6516,
      "step": 250
    },
    {
      "epoch": 0.42328042328042326,
      "grad_norm": 1.1240686178207397,
      "learning_rate": 4.1186440677966105e-05,
      "loss": 0.6828,
      "step": 260
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 0.8589078783988953,
      "learning_rate": 4.0762711864406784e-05,
      "loss": 0.6783,
      "step": 270
    },
    {
      "epoch": 0.45584045584045585,
      "grad_norm": 0.8516453504562378,
      "learning_rate": 4.0338983050847464e-05,
      "loss": 0.6743,
      "step": 280
    },
    {
      "epoch": 0.47212047212047215,
      "grad_norm": 0.7236848473548889,
      "learning_rate": 3.9915254237288136e-05,
      "loss": 0.6825,
      "step": 290
    },
    {
      "epoch": 0.4884004884004884,
      "grad_norm": 0.855140745639801,
      "learning_rate": 3.9491525423728816e-05,
      "loss": 0.671,
      "step": 300
    },
    {
      "epoch": 0.5046805046805046,
      "grad_norm": 0.8440850973129272,
      "learning_rate": 3.9067796610169495e-05,
      "loss": 0.6776,
      "step": 310
    },
    {
      "epoch": 0.5209605209605209,
      "grad_norm": 0.8390825390815735,
      "learning_rate": 3.8644067796610175e-05,
      "loss": 0.655,
      "step": 320
    },
    {
      "epoch": 0.5372405372405372,
      "grad_norm": 1.0086815357208252,
      "learning_rate": 3.8220338983050854e-05,
      "loss": 0.6647,
      "step": 330
    },
    {
      "epoch": 0.5535205535205535,
      "grad_norm": 0.7752877473831177,
      "learning_rate": 3.779661016949153e-05,
      "loss": 0.6534,
      "step": 340
    },
    {
      "epoch": 0.5698005698005698,
      "grad_norm": 0.9149221777915955,
      "learning_rate": 3.737288135593221e-05,
      "loss": 0.6938,
      "step": 350
    },
    {
      "epoch": 0.5860805860805861,
      "grad_norm": 0.8430819511413574,
      "learning_rate": 3.6949152542372886e-05,
      "loss": 0.6901,
      "step": 360
    },
    {
      "epoch": 0.6023606023606024,
      "grad_norm": 1.1142338514328003,
      "learning_rate": 3.6525423728813566e-05,
      "loss": 0.6709,
      "step": 370
    },
    {
      "epoch": 0.6186406186406186,
      "grad_norm": 0.8852651715278625,
      "learning_rate": 3.610169491525424e-05,
      "loss": 0.6449,
      "step": 380
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 0.994490921497345,
      "learning_rate": 3.567796610169492e-05,
      "loss": 0.7045,
      "step": 390
    },
    {
      "epoch": 0.6512006512006512,
      "grad_norm": 1.0016900300979614,
      "learning_rate": 3.52542372881356e-05,
      "loss": 0.6583,
      "step": 400
    },
    {
      "epoch": 0.6674806674806675,
      "grad_norm": 0.8443682789802551,
      "learning_rate": 3.483050847457627e-05,
      "loss": 0.6084,
      "step": 410
    },
    {
      "epoch": 0.6837606837606838,
      "grad_norm": 0.7933271527290344,
      "learning_rate": 3.440677966101695e-05,
      "loss": 0.6708,
      "step": 420
    },
    {
      "epoch": 0.7000407000407001,
      "grad_norm": 0.9349954724311829,
      "learning_rate": 3.398305084745763e-05,
      "loss": 0.6282,
      "step": 430
    },
    {
      "epoch": 0.7163207163207164,
      "grad_norm": 0.8797197341918945,
      "learning_rate": 3.35593220338983e-05,
      "loss": 0.6602,
      "step": 440
    },
    {
      "epoch": 0.7326007326007326,
      "grad_norm": 1.1528881788253784,
      "learning_rate": 3.313559322033898e-05,
      "loss": 0.6495,
      "step": 450
    },
    {
      "epoch": 0.7488807488807488,
      "grad_norm": 0.8779690861701965,
      "learning_rate": 3.271186440677966e-05,
      "loss": 0.6176,
      "step": 460
    },
    {
      "epoch": 0.7651607651607651,
      "grad_norm": 0.9756469130516052,
      "learning_rate": 3.228813559322034e-05,
      "loss": 0.6637,
      "step": 470
    },
    {
      "epoch": 0.7814407814407814,
      "grad_norm": 1.074717402458191,
      "learning_rate": 3.186440677966101e-05,
      "loss": 0.6784,
      "step": 480
    },
    {
      "epoch": 0.7977207977207977,
      "grad_norm": 0.9332597851753235,
      "learning_rate": 3.144067796610169e-05,
      "loss": 0.6646,
      "step": 490
    },
    {
      "epoch": 0.814000814000814,
      "grad_norm": 0.9236867427825928,
      "learning_rate": 3.101694915254237e-05,
      "loss": 0.6685,
      "step": 500
    },
    {
      "epoch": 0.8302808302808303,
      "grad_norm": 0.8163477778434753,
      "learning_rate": 3.059322033898305e-05,
      "loss": 0.6365,
      "step": 510
    },
    {
      "epoch": 0.8465608465608465,
      "grad_norm": 0.8803297281265259,
      "learning_rate": 3.016949152542373e-05,
      "loss": 0.6156,
      "step": 520
    },
    {
      "epoch": 0.8628408628408628,
      "grad_norm": 0.9088124632835388,
      "learning_rate": 2.9745762711864407e-05,
      "loss": 0.5982,
      "step": 530
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 0.8476722240447998,
      "learning_rate": 2.9322033898305083e-05,
      "loss": 0.6489,
      "step": 540
    },
    {
      "epoch": 0.8954008954008954,
      "grad_norm": 0.967504620552063,
      "learning_rate": 2.8898305084745763e-05,
      "loss": 0.625,
      "step": 550
    },
    {
      "epoch": 0.9116809116809117,
      "grad_norm": 0.7797608375549316,
      "learning_rate": 2.8474576271186442e-05,
      "loss": 0.6079,
      "step": 560
    },
    {
      "epoch": 0.927960927960928,
      "grad_norm": 1.1280207633972168,
      "learning_rate": 2.8050847457627122e-05,
      "loss": 0.6495,
      "step": 570
    },
    {
      "epoch": 0.9442409442409443,
      "grad_norm": 0.9664865136146545,
      "learning_rate": 2.7627118644067794e-05,
      "loss": 0.6095,
      "step": 580
    },
    {
      "epoch": 0.9605209605209605,
      "grad_norm": 0.9015458822250366,
      "learning_rate": 2.7203389830508474e-05,
      "loss": 0.6026,
      "step": 590
    },
    {
      "epoch": 0.9768009768009768,
      "grad_norm": 0.8975563049316406,
      "learning_rate": 2.6779661016949153e-05,
      "loss": 0.6092,
      "step": 600
    },
    {
      "epoch": 0.9930809930809931,
      "grad_norm": 0.9935470223426819,
      "learning_rate": 2.6355932203389833e-05,
      "loss": 0.6347,
      "step": 610
    },
    {
      "epoch": 1.008140008140008,
      "grad_norm": 1.0726003646850586,
      "learning_rate": 2.5932203389830512e-05,
      "loss": 0.5523,
      "step": 620
    },
    {
      "epoch": 1.0244200244200243,
      "grad_norm": 1.1311161518096924,
      "learning_rate": 2.5508474576271185e-05,
      "loss": 0.6116,
      "step": 630
    },
    {
      "epoch": 1.0407000407000406,
      "grad_norm": 1.0107568502426147,
      "learning_rate": 2.5084745762711865e-05,
      "loss": 0.5952,
      "step": 640
    },
    {
      "epoch": 1.056980056980057,
      "grad_norm": 0.9949808716773987,
      "learning_rate": 2.4661016949152544e-05,
      "loss": 0.592,
      "step": 650
    },
    {
      "epoch": 1.0732600732600732,
      "grad_norm": 1.0277072191238403,
      "learning_rate": 2.4237288135593224e-05,
      "loss": 0.6078,
      "step": 660
    },
    {
      "epoch": 1.0895400895400895,
      "grad_norm": 1.2753901481628418,
      "learning_rate": 2.38135593220339e-05,
      "loss": 0.5887,
      "step": 670
    },
    {
      "epoch": 1.1058201058201058,
      "grad_norm": 0.9852465987205505,
      "learning_rate": 2.338983050847458e-05,
      "loss": 0.6013,
      "step": 680
    },
    {
      "epoch": 1.122100122100122,
      "grad_norm": 1.1314862966537476,
      "learning_rate": 2.2966101694915255e-05,
      "loss": 0.596,
      "step": 690
    },
    {
      "epoch": 1.1383801383801384,
      "grad_norm": 1.3713140487670898,
      "learning_rate": 2.2542372881355935e-05,
      "loss": 0.6251,
      "step": 700
    },
    {
      "epoch": 1.1546601546601547,
      "grad_norm": 1.4215116500854492,
      "learning_rate": 2.211864406779661e-05,
      "loss": 0.5935,
      "step": 710
    },
    {
      "epoch": 1.170940170940171,
      "grad_norm": 1.1472182273864746,
      "learning_rate": 2.1694915254237287e-05,
      "loss": 0.6179,
      "step": 720
    },
    {
      "epoch": 1.1872201872201873,
      "grad_norm": 1.1714472770690918,
      "learning_rate": 2.1271186440677967e-05,
      "loss": 0.6267,
      "step": 730
    },
    {
      "epoch": 1.2035002035002036,
      "grad_norm": 1.2228699922561646,
      "learning_rate": 2.0847457627118643e-05,
      "loss": 0.6467,
      "step": 740
    },
    {
      "epoch": 1.2197802197802199,
      "grad_norm": 1.132529616355896,
      "learning_rate": 2.0423728813559322e-05,
      "loss": 0.6233,
      "step": 750
    },
    {
      "epoch": 1.236060236060236,
      "grad_norm": 1.018088936805725,
      "learning_rate": 2e-05,
      "loss": 0.5637,
      "step": 760
    },
    {
      "epoch": 1.2523402523402523,
      "grad_norm": 1.273481011390686,
      "learning_rate": 1.9576271186440678e-05,
      "loss": 0.6326,
      "step": 770
    },
    {
      "epoch": 1.2686202686202686,
      "grad_norm": 1.1115964651107788,
      "learning_rate": 1.9152542372881357e-05,
      "loss": 0.5679,
      "step": 780
    },
    {
      "epoch": 1.2849002849002849,
      "grad_norm": 1.3014588356018066,
      "learning_rate": 1.8728813559322033e-05,
      "loss": 0.6204,
      "step": 790
    },
    {
      "epoch": 1.3011803011803011,
      "grad_norm": 0.9897966980934143,
      "learning_rate": 1.8305084745762713e-05,
      "loss": 0.5877,
      "step": 800
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 1.456350326538086,
      "learning_rate": 1.7881355932203392e-05,
      "loss": 0.6329,
      "step": 810
    },
    {
      "epoch": 1.3337403337403337,
      "grad_norm": 1.0288900136947632,
      "learning_rate": 1.745762711864407e-05,
      "loss": 0.5758,
      "step": 820
    },
    {
      "epoch": 1.35002035002035,
      "grad_norm": 1.0738714933395386,
      "learning_rate": 1.7033898305084748e-05,
      "loss": 0.6185,
      "step": 830
    },
    {
      "epoch": 1.3663003663003663,
      "grad_norm": 1.1994534730911255,
      "learning_rate": 1.6610169491525424e-05,
      "loss": 0.6077,
      "step": 840
    },
    {
      "epoch": 1.3825803825803826,
      "grad_norm": 1.1989309787750244,
      "learning_rate": 1.6186440677966104e-05,
      "loss": 0.6045,
      "step": 850
    },
    {
      "epoch": 1.398860398860399,
      "grad_norm": 1.3433303833007812,
      "learning_rate": 1.5762711864406783e-05,
      "loss": 0.6103,
      "step": 860
    },
    {
      "epoch": 1.4151404151404152,
      "grad_norm": 1.3200175762176514,
      "learning_rate": 1.533898305084746e-05,
      "loss": 0.6295,
      "step": 870
    },
    {
      "epoch": 1.4314204314204315,
      "grad_norm": 1.264816164970398,
      "learning_rate": 1.4915254237288137e-05,
      "loss": 0.5929,
      "step": 880
    },
    {
      "epoch": 1.4477004477004476,
      "grad_norm": 1.4000226259231567,
      "learning_rate": 1.4491525423728813e-05,
      "loss": 0.5675,
      "step": 890
    },
    {
      "epoch": 1.463980463980464,
      "grad_norm": 1.149515986442566,
      "learning_rate": 1.4067796610169493e-05,
      "loss": 0.5844,
      "step": 900
    },
    {
      "epoch": 1.4802604802604802,
      "grad_norm": 1.3389369249343872,
      "learning_rate": 1.3644067796610169e-05,
      "loss": 0.5833,
      "step": 910
    },
    {
      "epoch": 1.4965404965404965,
      "grad_norm": 1.4920849800109863,
      "learning_rate": 1.3220338983050848e-05,
      "loss": 0.5626,
      "step": 920
    },
    {
      "epoch": 1.5128205128205128,
      "grad_norm": 1.3542208671569824,
      "learning_rate": 1.2796610169491528e-05,
      "loss": 0.5895,
      "step": 930
    },
    {
      "epoch": 1.529100529100529,
      "grad_norm": 1.066347360610962,
      "learning_rate": 1.2372881355932204e-05,
      "loss": 0.5771,
      "step": 940
    },
    {
      "epoch": 1.5453805453805454,
      "grad_norm": 1.1147923469543457,
      "learning_rate": 1.1949152542372882e-05,
      "loss": 0.5722,
      "step": 950
    },
    {
      "epoch": 1.5616605616605617,
      "grad_norm": 1.314718246459961,
      "learning_rate": 1.152542372881356e-05,
      "loss": 0.5611,
      "step": 960
    },
    {
      "epoch": 1.577940577940578,
      "grad_norm": 1.195533037185669,
      "learning_rate": 1.1101694915254237e-05,
      "loss": 0.5847,
      "step": 970
    },
    {
      "epoch": 1.5942205942205943,
      "grad_norm": 1.2239032983779907,
      "learning_rate": 1.0677966101694915e-05,
      "loss": 0.6038,
      "step": 980
    },
    {
      "epoch": 1.6105006105006106,
      "grad_norm": 1.2657185792922974,
      "learning_rate": 1.0254237288135593e-05,
      "loss": 0.5921,
      "step": 990
    },
    {
      "epoch": 1.6267806267806266,
      "grad_norm": 1.2853691577911377,
      "learning_rate": 9.830508474576272e-06,
      "loss": 0.5673,
      "step": 1000
    },
    {
      "epoch": 1.6430606430606431,
      "grad_norm": 1.182492733001709,
      "learning_rate": 9.40677966101695e-06,
      "loss": 0.5641,
      "step": 1010
    },
    {
      "epoch": 1.6593406593406592,
      "grad_norm": 1.6746176481246948,
      "learning_rate": 8.983050847457628e-06,
      "loss": 0.6232,
      "step": 1020
    },
    {
      "epoch": 1.6756206756206757,
      "grad_norm": 1.266327142715454,
      "learning_rate": 8.559322033898306e-06,
      "loss": 0.6159,
      "step": 1030
    },
    {
      "epoch": 1.6919006919006918,
      "grad_norm": 1.2442492246627808,
      "learning_rate": 8.135593220338983e-06,
      "loss": 0.6103,
      "step": 1040
    },
    {
      "epoch": 1.7081807081807083,
      "grad_norm": 1.2691539525985718,
      "learning_rate": 7.711864406779661e-06,
      "loss": 0.5875,
      "step": 1050
    },
    {
      "epoch": 1.7244607244607244,
      "grad_norm": 1.8338905572891235,
      "learning_rate": 7.28813559322034e-06,
      "loss": 0.5669,
      "step": 1060
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 1.3122360706329346,
      "learning_rate": 6.864406779661017e-06,
      "loss": 0.6186,
      "step": 1070
    },
    {
      "epoch": 1.757020757020757,
      "grad_norm": 1.3464545011520386,
      "learning_rate": 6.440677966101695e-06,
      "loss": 0.6026,
      "step": 1080
    },
    {
      "epoch": 1.7733007733007733,
      "grad_norm": 1.32087242603302,
      "learning_rate": 6.016949152542373e-06,
      "loss": 0.5882,
      "step": 1090
    },
    {
      "epoch": 1.7895807895807896,
      "grad_norm": 1.2928547859191895,
      "learning_rate": 5.593220338983051e-06,
      "loss": 0.5712,
      "step": 1100
    },
    {
      "epoch": 1.8058608058608059,
      "grad_norm": 1.2825933694839478,
      "learning_rate": 5.169491525423729e-06,
      "loss": 0.5996,
      "step": 1110
    },
    {
      "epoch": 1.8221408221408222,
      "grad_norm": 1.2754724025726318,
      "learning_rate": 4.745762711864407e-06,
      "loss": 0.5523,
      "step": 1120
    },
    {
      "epoch": 1.8384208384208383,
      "grad_norm": 1.3911138772964478,
      "learning_rate": 4.3220338983050846e-06,
      "loss": 0.5723,
      "step": 1130
    },
    {
      "epoch": 1.8547008547008548,
      "grad_norm": 1.5861175060272217,
      "learning_rate": 3.898305084745763e-06,
      "loss": 0.5708,
      "step": 1140
    },
    {
      "epoch": 1.8709808709808708,
      "grad_norm": 1.2816712856292725,
      "learning_rate": 3.474576271186441e-06,
      "loss": 0.6108,
      "step": 1150
    },
    {
      "epoch": 1.8872608872608874,
      "grad_norm": 1.2848154306411743,
      "learning_rate": 3.050847457627119e-06,
      "loss": 0.5918,
      "step": 1160
    },
    {
      "epoch": 1.9035409035409034,
      "grad_norm": 1.3733961582183838,
      "learning_rate": 2.6271186440677966e-06,
      "loss": 0.5942,
      "step": 1170
    },
    {
      "epoch": 1.91982091982092,
      "grad_norm": 1.13031804561615,
      "learning_rate": 2.203389830508475e-06,
      "loss": 0.5947,
      "step": 1180
    },
    {
      "epoch": 1.936100936100936,
      "grad_norm": 1.5267702341079712,
      "learning_rate": 1.7796610169491526e-06,
      "loss": 0.6091,
      "step": 1190
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 1.3209587335586548,
      "learning_rate": 1.3559322033898304e-06,
      "loss": 0.5977,
      "step": 1200
    },
    {
      "epoch": 1.9686609686609686,
      "grad_norm": 1.2461086511611938,
      "learning_rate": 9.322033898305086e-07,
      "loss": 0.5957,
      "step": 1210
    },
    {
      "epoch": 1.984940984940985,
      "grad_norm": 1.8259265422821045,
      "learning_rate": 5.084745762711865e-07,
      "loss": 0.5714,
      "step": 1220
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.735836386680603,
      "learning_rate": 8.47457627118644e-08,
      "loss": 0.5528,
      "step": 1230
    }
  ],
  "logging_steps": 10,
  "max_steps": 1230,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0580223506153472e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
